\section{Conclusion}
\label{conclusion}

In this paper we presented a study of the performance inefficiencies in scientific code, using a particle reconstruction analysis application as a case study. Top quark and Higgs boson studies require reconstructing form measurements of a very large number of particle collisions, performed weekly by the \tth application in terabytes of data. A faster and more refined analysis of the data allows to better reconstruct more particle collisions and improve the quality of the physics research.

We identified and removed inefficiencies in different stages of the application. In the code design, tackling inefficiencies in the pseudo-random number generation, a common component of most simulation and analysis applications, which provided a 71\% increase in performance. In data structure design, by analysing the factors limiting the particle reconstruction parallelization, and presenting and testing two different solutions for shared memory environments. The former, with a constant scaling on a dual-socket NUMA system, providing a maximum speedup of 8.8, while the latter, more efficient, with a peak speedup of 5.8 but only using one CPU device, compared to the optimized application. Finally, at application runtime, where a multiprocess approach using a more efficient parallel implementation was introduced to tackle its inefficiencies on NUMA systems, providing a maximum speedup of 69.3. An efficient control on the thread affinity of the more efficient parallel implementation and multiprocess approach, provided improved performances for 512 and 1024 variations, with a peak improvements up to 37\% and 90\%, respectively. However, the fluctuation in performance and the dependencies on many system characteristics prevent providing a generalized heuristic to aid to control the best affinity for the application, whatever the computing system.

We hope to improve the sensibility of scientists to the efficiency pitfalls common in scientific code, to help develop more efficient and performing applications. The performance of the \tth application was improved by a factor of 113, for the test system used, helping physicists to execute more particle collisions with a more refined reconstruction process, which efficiently uses the available computational resources.

Even with the interesting improvements to the application efficiency, some directions of future research were identified. The scheduler could be improved to automatically predict the best process/thread configuration for each system by analysing a set of micro-benchmarks or the application itself on a small input, and ultimately identify the best core affinity scheme. Also, the application efficiency could be improved using hardware accelerators, balancing the workload among accelerators and CPU devices in heterogeneous systems. Frameworks for parallelization and workload balancing for heterogeneous systems can be analysed and tested with this case study.
