\section{Introduction}
\label{introduction}

The European Organization for Nuclear Research (CERN) is a consortium of 20 European countries aiming to operate the largest High Energy Physics (HEP) experiments in the world. The instrumentation used in nuclear and particle physics research is essentially formed by particle accelerators and detectors. The Large Hadron Collider (LHC) particle accelerator speeds up groups of particles close to the speed of light, in opposite directions, inducing a controlled collision the detectors core. The detectors record various characteristics of the resultant particles of each collision (an event), such as energy and momentum, which originate from complex decay processes of the collided protons. The purpose of these experiments is to test the HEP theories, such as the Standard Model, by confirming or discovering new particles and physics.

The ATLAS experiment is one of the seven particle detectors at CERN whose main goals are to confirm the existence of the top quarks and the Higgs boson in the Standard model. Approximately 600 million collisions occur every second at the LHC. Particles produced in head-on proton collisions interact with the detectors, generating massive amounts of raw data. It is estimated that all the combined detectors produce 25 petabytes of data per year, and it is expected to grow after the ongoing LHC upgrade \cite{LIP:Ibergrid}. This data then passes a set of processing and refinement stages until it is ready to be used to reconstruct the events by specific data analysis code, which may vary according to the HEP theory being researched. Several research groups work in event reconstruction in the same experiment, enforcing positive competition to produce quality results in a fast and consistent way.

These factors enforce the need to process more data, more accurately, in less time. Research groups often opt to invest on larger computing clusters to improve the quality of their research results. However, most scientific code was not designed and/or developed for an efficient use of the available computational resources. If these applications were adequately tuned (or redesigned), the event analysis throughput could be massively increased. An efficient parallel application can significantly improve its performance at a much lower cost, as shown in \cite{Msc:AMP}.

This paper addresses inefficiencies in two stages of the data analysis application: the code development and application runtime. In the former, inefficiencies in the algorithm coding and data structuring are pinpointed and several solutions are suggested, based on a quantitative analysis of the bottlenecks. The latter identifies inefficiencies in threads accessing remote shared memory, and gives hints to overcome these limitations.

This paper is organized as follows: section \ref{problem_and_app} briefly presents the top quark and Higgs boson decay process and introduces a short characterization of the data analysis application used as case study; in section \ref{identification} the code inefficiencies are identified, analysed, and removed, with a final shared memory parallelization proposal; in section \ref{removal}, runtime inefficiencies of the parallelization are identified and possible alternatives suggested, concluding with an assessment of the core affinity impact; finally, section \ref{conclusion} concludes the paper and proposes some future work.
