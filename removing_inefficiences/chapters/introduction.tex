\section{Introduction}
\label{introduction}

The European Organization for Nuclear Research (CERN) is a consortium of 20 European countries, founded in 1954, with the purpose of operating the largest High Energy Physics (HEP) experiments in the world. The instrumentation used in nuclear and particle physics research is essentially divided into particle accelerators and detectors. The Large Hadron Collider (LHC) particle accelerator speeds up groups of particles close to the speed of light, in opposite directions, resulting in a controlled collision inside the detectors (each collision is considered an event). The detectors record various characteristics of the resultant particles, such as energy and momentum, which originate from complex decay processes of the collided protons. The purpose of these experiments is to test the HEP theories, such as the Standard Model, by confirming or discovering new particles and physics.

The ATLAS experiment, a key project at CERN, is conducting most of the crucial experiments on both the top quark and Higgs boson measurements. Approximately 600 millions of collisions occur every second at the LHC. Particles produced in head-on proton collisions interact with the detectors of the ATLAS experiment, generating massive amounts of raw data as electric signals. It is estimated that all the detectors combined produce 25 petabytes of data per year, and it is expected to grow after the LHC upgrade, which purpose is to increase the amount of energy of the accelerated particle beams. This data passes a set of processing and refinement until it is ready to be used to reconstruct the events by specific applications. The application varies according to the HEP theory being analysed. At this stage, different research groups in the same experiment enforce positive competition to produce quality results in a fast and consistent way.

These factors enforce the need to process more data, more accurately, and in less time. Research groups often opt to increase computational resources in a brute force approach to improve their research quality. However, applications use inefficiently the available computational resources, so, if properly optimized, the computational throughput could be highly boosted. A properly optimized application can provide an equal or greater performance increase at a much lower cost. This paper aims to provide a set of techniques to identify and remove inefficiencies in scientific applications, using a top quark and Higgs boson analysis application as a case study, to help scientist develop and optimize applications that efficiently use the available resources.

This paper is organized as follows: section \ref{problem_and_app} briefly presents the top quark and Higgs boson decay process and introduces a short characterization of the \tth application used as case study; section \ref{identification} presents and characterises the inefficiencies identified in the application; section \ref{removal} shows the process of removing the inefficiencies and optimizing the application at development and runtime stages; finally, section \ref{conclusion} concludes the paper and mentions future work.
